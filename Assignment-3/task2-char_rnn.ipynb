{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char-RNN implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. This network is first posted by Andrej Karpathy, you can find out about his original code on https://github.com/karpathy/char-rnn, the original code is written in *lua*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement Char-RNN using Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup\n",
    "In this part, we will read the data of our input text and process the text for later network training. There are two txt files in the data folder, for computing time consideration, we will use tinyshakespeare.txt here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('data/tinyshakespeare.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "# and let's get a glance of what the text is\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n",
      "First Citi --- characters mapped to int --- > [18 47 56 57 58  1 15 47 58 47]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "vocab_to_ind = {c: i for i, c in enumerate(vocab)}\n",
    "ind_to_vocab = dict(enumerate(vocab))\n",
    "text_as_int = np.array([vocab_to_ind[c] for c in text], dtype=np.int32)\n",
    "\n",
    "# We mapped the character as indexes from 0 to len(vocab)\n",
    "for char,_ in zip(vocab_to_ind, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), vocab_to_ind[char]))\n",
    "# Show how the first 10 characters from the text are mapped to integers\n",
    "print ('{} --- characters mapped to int --- > {}'.format(text[:10], text_as_int[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating batches\n",
    "Now that we have preprocessed our input data, we then need to partition our data, here we will use mini-batches to train our model, so how will we define our batches?\n",
    "\n",
    "Let's first clarify the concepts of batches:\n",
    "1. **batch_size**: Reviewing batches in CNN, if we have 100 samples and we set batch_size as 10, it means that we will send 10 samples to the network at one time. In RNN, batch_size have the same meaning, it defines how many samples we send to the network at one time.\n",
    "2. **sequence_length**: However, as for RNN, we store memory in our cells, we pass the information through cells, so we have this sequence_length concept, which also called 'steps', it defines how long a sequence is.\n",
    "\n",
    "From above two concepts, we here clarify the meaning of batch_size in RNN. Here, we define the number of sequences in a batch as N and the length of each sequence as M, so batch_size in RNN **still** represent the number of sequences in a batch but the data size of a batch is actually an array of size **[N, M]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish the get_batches() function below to generate mini-batches.\n",
    "\n",
    "Hint: this function defines a generator, use *yield*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(array, n_seqs, n_steps):\n",
    "    '''\n",
    "    Partition data array into mini-batches\n",
    "    input:\n",
    "    array: input data\n",
    "    n_seqs: number of sequences in a batch\n",
    "    n_steps: length of each sequence\n",
    "    output:\n",
    "    x: inputs\n",
    "    y: targets, which is x with one position shift\n",
    "       you can check the following figure to get the sence of what a target looks like\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(array) / batch_size)\n",
    "    # we only keep the full batches and ignore the left.\n",
    "    array = array[:batch_size * n_batches]\n",
    "    array = array.reshape((n_seqs, -1))\n",
    "    batch_count =0\n",
    "    while(True) :\n",
    "        if batch_count >= n_batches :\n",
    "            batch_count =0\n",
    "            \n",
    "        else :\n",
    "            \n",
    "            x = array[:,batch_count*n_seqs : (n_seqs)*(batch_count+1)]\n",
    "            y =np.roll(x, -1, axis =1)\n",
    "            batch_count +=1\n",
    "            yield[x,y]\n",
    "           \n",
    "        \n",
    "    \n",
    "    # You should now create a loop to generate batches for inputs and targets\n",
    "    #############################################\n",
    "    #           TODO: YOUR CODE HERE            #\n",
    "    #############################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[18 47 56 57 58  1 15 47 58 47]\n",
      " [ 1 43 52 43 51 63 11  0 37 43]\n",
      " [52 58 43 42  1 60 47 56 58 59]\n",
      " [56 44 53 50 49  6  0 27 52  1]\n",
      " [47 52  1 57 54 47 58 43  1 53]\n",
      " [56 57  6  1 39 52 42  1 57 58]\n",
      " [46 47 51  1 42 53 61 52  1 58]\n",
      " [ 1 40 43 43 52  1 57 47 52 41]\n",
      " [50 58 57  1 51 39 63  1 57 46]\n",
      " [57 47 53 52  1 53 44  1 56 43]]\n",
      "\n",
      "y\n",
      " [[47 56 57 58  1 15 47 58 47 18]\n",
      " [43 52 43 51 63 11  0 37 43  1]\n",
      " [58 43 42  1 60 47 56 58 59 52]\n",
      " [44 53 50 49  6  0 27 52  1 56]\n",
      " [52  1 57 54 47 58 43  1 53 47]\n",
      " [57  6  1 39 52 42  1 57 58 56]\n",
      " [47 51  1 42 53 61 52  1 58 46]\n",
      " [40 43 43 52  1 57 47 52 41  1]\n",
      " [58 57  1 51 39 63  1 57 46 50]\n",
      " [47 53 52  1 53 44  1 56 43 57]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(text_as_int, 10, 10)\n",
    "x,y= next(batches)\n",
    "\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Char-RNN model\n",
    "In this section, we will build our char-rnn model, it consists of input layer, rnn_cell layer, output layer, loss and optimizer, we will build them one by one.\n",
    "\n",
    "The goal is to predict new text after given prime word, so for our training data, we have to define inputs and targets, here is a figure that explains the structure of the Char-RNN network.\n",
    "\n",
    "![structure](img/charrnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish all TODOs in ecbm4040.CharRNN and the blanks in the following cells.\n",
    "\n",
    "**Note: The training process on following settings of parameters takes about 20 minutes on a GTX 1070 GPU, so you are suggested to use GCP for this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.CharRNN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Set sampling as False(default), we can start training the network, we automatically save checkpoints in the folder /checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "step: 200  loss: 2.1968  0.1872 sec/batch\n",
      "step: 400  loss: 1.8883  0.1889 sec/batch\n",
      "step: 600  loss: 1.7444  0.1873 sec/batch\n",
      "step: 800  loss: 1.6774  0.1884 sec/batch\n",
      "step: 1000  loss: 1.7117  0.1886 sec/batch\n",
      "step: 1200  loss: 1.5788  0.1865 sec/batch\n",
      "step: 1400  loss: 1.5539  0.1879 sec/batch\n",
      "step: 1600  loss: 1.5088  0.1888 sec/batch\n",
      "step: 1800  loss: 1.4735  0.1904 sec/batch\n",
      "step: 2000  loss: 1.4951  0.1873 sec/batch\n",
      "step: 2200  loss: 1.4805  0.1883 sec/batch\n",
      "step: 2400  loss: 1.4652  0.1888 sec/batch\n",
      "step: 2600  loss: 1.4783  0.1880 sec/batch\n",
      "step: 2800  loss: 1.4468  0.1842 sec/batch\n",
      "step: 3000  loss: 1.4143  0.1866 sec/batch\n",
      "step: 3200  loss: 1.4518  0.1872 sec/batch\n",
      "step: 3400  loss: 1.3825  0.1862 sec/batch\n",
      "step: 3600  loss: 1.4363  0.1886 sec/batch\n",
      "step: 3800  loss: 1.4071  0.1881 sec/batch\n",
      "step: 4000  loss: 1.4141  0.1859 sec/batch\n",
      "step: 4200  loss: 1.3862  0.1845 sec/batch\n",
      "step: 4400  loss: 1.3868  0.1884 sec/batch\n",
      "step: 4600  loss: 1.3861  0.1864 sec/batch\n",
      "step: 4800  loss: 1.3870  0.1884 sec/batch\n",
      "step: 5000  loss: 1.3531  0.1882 sec/batch\n",
      "step: 5200  loss: 1.3574  0.1889 sec/batch\n",
      "step: 5400  loss: 1.3684  0.1875 sec/batch\n",
      "step: 5600  loss: 1.4037  0.1880 sec/batch\n",
      "step: 5800  loss: 1.3581  0.1880 sec/batch\n",
      "step: 6000  loss: 1.3501  0.1872 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "\"\"\"\n",
    "x,y = next(batches)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "print (x.shape)\n",
    "print(y.shape)\n",
    "\"\"\"\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/gi6000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/lstmi2000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/lstmi4000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/lstmi6000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/gi6000_l256.ckpt\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Set the sampling as True and we can generate new characters one by one. We can use our saved checkpoints to see how the network learned gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/gi6000_l256.ckpt\n",
      "SCALIST:\n",
      "That tend the cirture of an issue in all;\n",
      "And we have something but something some marks,\n",
      "As sorrow he is then, to bid him with all,\n",
      "When will not say I have an ancely,\n",
      "I am born a shall thou art heart withal.\n",
      "\n",
      "GLOUCESTER:\n",
      "Then, with a propers and shore steps and high.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I'll hear the selfsit fire of something sometime,\n",
      "True words of subject, wert I say'st.\n",
      "\n",
      "LEONTES:\n",
      "When he did throw him that he was at them,\n",
      "But the more fealties of your strong infinite\n",
      "As thou destroy with the part of that;\n",
      "We have not the painter with me with a soul,\n",
      "Art any on their words, that thou art to thee.\n",
      "\n",
      "KING EDWARD IV:\n",
      "The ways are both in tears were as an istue,\n",
      "Thy such a man of heart to me and so,\n",
      "Which tell me here as true worse wanters,\n",
      "As I spoke in my house against them spoke,\n",
      "To honour a sence that I have done the means;\n",
      "For that, and tell this service in to the world,\n",
      "The presently that terror of this witness to sea,\n",
      "Is all a wife.\n",
      "\n",
      "CLARENCE:\n",
      "When I have hat me to thyself?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps,'LSTM', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/lstmi2000_l256.ckpt\n",
      "GLOUCEST:\n",
      "In the stands out and the compronient as the\n",
      "sheather be to bein to her from my heart;\n",
      "As we shall hear her hand of more will be,\n",
      "And that a pardon and my light women,\n",
      "I have shown her to me the parchous blood,\n",
      "And shis to see a march, that, see the post\n",
      "What streight of my some world or so with the\n",
      "prisoner, by my suberied, when the death of shraigh,\n",
      "In army that me so the souls of my\n",
      "thing the winds of her take,\n",
      "We shall she will be married, and the might\n",
      "To the weeth and son, we were to be served,\n",
      "To may so starms that the some of her sun\n",
      "Which though and threaten weary weether'd bare\n",
      "That the stands of her with her, with her best,\n",
      "To the winding thinks a bean and breast them answer.\n",
      "\n",
      "PETRUCHIO:\n",
      "We have a bride to make you hence, and true to\n",
      "the children to yourselves: by this till they have,\n",
      "What warring her of harks: what's she'll stay:\n",
      "I shall be mean as whene we may have this?\n",
      "\n",
      "BENVOLIO:\n",
      "I have no more of mine to so to them;\n",
      "Would hear him be the son.\n",
      "\n",
      "POLIXENES:\n",
      "I do n\n"
     ]
    }
   ],
   "source": [
    "# choose a checkpoint other than the final one and see the results. It could be nasty, don't worry!\n",
    "#############################################\n",
    "#           TODO: YOUR CODE HERE            #\n",
    "#############################################\n",
    "checkpoint = 'checkpoints/lstmi2000_l256.ckpt'\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change another type of RNN cell\n",
    "We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell in rnn_cell layer to GRU cell and see how it performs. Your number of step should be the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to change your saved checkpoints' name or they will rewrite the LSTM results that you have already saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "step: 200  loss: 3.2987  0.1771 sec/batch\n",
      "step: 400  loss: 2.5488  0.1785 sec/batch\n",
      "step: 600  loss: 2.3004  0.1770 sec/batch\n",
      "step: 800  loss: 2.1526  0.1764 sec/batch\n",
      "step: 1000  loss: 2.0516  0.1779 sec/batch\n",
      "step: 1200  loss: 1.8370  0.1764 sec/batch\n",
      "step: 1400  loss: 1.7286  0.1773 sec/batch\n",
      "step: 1600  loss: 1.6566  0.1768 sec/batch\n",
      "step: 1800  loss: 1.5781  0.1758 sec/batch\n",
      "step: 2000  loss: 1.5982  0.1760 sec/batch\n",
      "step: 2200  loss: 1.5710  0.1757 sec/batch\n",
      "step: 2400  loss: 1.5141  0.1776 sec/batch\n",
      "step: 2600  loss: 1.5446  0.1746 sec/batch\n",
      "step: 2800  loss: 1.5011  0.1749 sec/batch\n",
      "step: 3000  loss: 1.4587  0.1759 sec/batch\n",
      "step: 3200  loss: 1.4676  0.1766 sec/batch\n",
      "step: 3400  loss: 1.4318  0.1764 sec/batch\n",
      "step: 3600  loss: 1.4696  0.1741 sec/batch\n",
      "step: 3800  loss: 1.4353  0.1748 sec/batch\n",
      "step: 4000  loss: 1.4404  0.1764 sec/batch\n",
      "step: 4200  loss: 1.4229  0.1757 sec/batch\n",
      "step: 4400  loss: 1.3997  0.1760 sec/batch\n",
      "step: 4600  loss: 1.4188  0.1753 sec/batch\n",
      "step: 4800  loss: 1.4181  0.1746 sec/batch\n",
      "step: 5000  loss: 1.3861  0.1760 sec/batch\n",
      "step: 5200  loss: 1.3757  0.1756 sec/batch\n",
      "step: 5400  loss: 1.3784  0.1754 sec/batch\n",
      "step: 5600  loss: 1.4323  0.1760 sec/batch\n",
      "step: 5800  loss: 1.3861  0.1753 sec/batch\n",
      "step: 6000  loss: 1.3520  0.1747 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/gi6000_l256.ckpt\n",
      "LAUD:\n",
      "That's some poor sense; thy sorrows shall be so:\n",
      "For that I see so, to thyself a sease.\n",
      "\n",
      "DUME IO:\n",
      "Would I be so all the chair of me?\n",
      "\n",
      "BALOR:\n",
      "If they be said the words at this that doth not\n",
      "And well that is the deod of anger it.\n",
      "I shall not be a man and mine in thee;\n",
      "And that the sorrows to be stored weiker,\n",
      "Thoreself see so shall so disgracious mards.\n",
      "\n",
      "BUSIS:\n",
      "O care that didst thou'll be thy back in me;\n",
      "Which is my blood to me, to do thy sake.\n",
      "\n",
      "JULIET:\n",
      "O my save, and my wounds thy sun of this.\n",
      "I am another, for so say that seems,\n",
      "Though thy begest still but first: and there shall be\n",
      "To-morrows that I do; and we have seen a better\n",
      "As seem the foot are stopped of a boot.\n",
      "He was a marrion is my served men.\n",
      "Heaven do me sends, I shall do now.\n",
      "To many seems, and whose sorrow hath set not\n",
      "One that his hand and marry such her brothers.\n",
      "\n",
      "DUKE OF YORK:\n",
      "We'll buse a possible.\n",
      "\n",
      "BUCKINGHAM:\n",
      "But when you honour, sir, to some all the hands:\n",
      "The gracious state of me, take a man, thou shalt not,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Compare your result of two networks that you built and the reasons that caused the difference. (It is a qualitative comparison, it should be based on the specific model that you build.)\n",
    "\n",
    "Results :\n",
    "\n",
    "LSTM: loss reduced to ~ 1.35\n",
    "GRU: loss reduced to ~1.35\n",
    "\n",
    "LSTM sees a faster reduction in loss compared to the GRU cell,due to more capability to capture a function and more paramters that are able to more quickly recognize and train itself to the function.\n",
    "\n",
    "LSTM : separate output and hidden\n",
    "GRU : output and hidden same\n",
    "\n",
    "2. Discuss the difference between LSTM cells and GRU cells, what are the pros and cons of using GRU cells?\n",
    "\n",
    "Pros of GRU Cell:\n",
    "- Lesser memory\n",
    "- Lesser time to run (train)\n",
    "- Lesser parameters to train\n",
    "\n",
    "Cons of GRU Cell:\n",
    "- Lesser capalitity to capture a function.\n",
    "- Takes longer iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
